import streamlit as st
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import altair as alt
import time

# Function to fetch data (simulated for demonstration)
def fetch_data(provincecode, policestationcode, year, quarter):
    # Simulated data fetching process
    data = {
        'Date': pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D'),
        'CrimeCategory': np.random.choice(['Theft', 'Assault', 'Burglary'], size=365),
        'Count': np.random.randint(0, 10, size=365)
    }
    return data

# Page title
st.set_page_config(page_title='Crime Hotspot Interpretable ML Model Building', page_icon='ðŸ¤–', layout='wide')
st.title('ðŸ¤– Crime Hotspot Interpretable ML Model Building')

with st.expander('About this app'):
    st.markdown('**What can this app do?**')
    st.info('This app allows users to build a machine learning (ML) model in an end-to-end workflow. '
            'Particularly, this encompasses data upload, data pre-processing, ML model building, '
            'and post-model analysis.')

    st.markdown('**How to use the app?**')
    st.warning('To engage with the app, go to the sidebar and 1. Select a data set and 2. Adjust the model '
               'parameters by adjusting the various slider widgets. As a result, this would initiate the '
               'ML model building process, display the model results as well as allowing users to download '
               'the generated models and accompanying data.')

    st.markdown('**Under the hood**')
    st.markdown('Data sets:')
    st.code('- Crime data set South Africa', language='markdown')
  
    st.markdown('Libraries used:')
    st.code('''- Pandas for data wrangling
- Scikit-learn for building a machine learning model
- Altair for chart creation
- Streamlit for user interface
    ''', language='markdown')

# Sidebar for accepting input parameters
quarter = st.radio('Select quarter of year', options=[1, 2, 3, 4], index=0)

with st.sidebar:
    st.header('1. Input data')

    # Initialize df
    df = pd.DataFrame()

    st.markdown('**1. Use custom data**')
    uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, index_col=False)
    else:
        st.info('Upload a CSV file to load data.')

    # Download example data
    example_csv = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')
    csv = example_csv.to_csv(index=False).encode('utf-8')

    st.download_button(
        label="Download example CSV",
        data=csv,
        file_name='delaney_solubility_with_descriptors.csv',
        mime='text/csv',
    )

    # Fetch data based on inputs
    st.header('2. Fetch Data')
    example_data = st.toggle('Use pre-populated csv data')
    if example_data:
        df = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')
    else:
        with st.expander('Select Input Parameters'):
            province_mapping = {
                'Western Cape': 'ZA.WC',
                'Eastern Cape': 'ZA.EC',
                'Free State': 'ZA.FS',
                'Gauteng': 'ZA.GP',
                'Mpumalanga': 'ZA.MP',
                'Northern Cape': 'ZA.NC',
                'KwaZulu-Natal': 'ZA.NL',
                'Limpopo': 'ZA.NP',
                'North-West': 'ZA.NW',
            }
            provincecode = st.selectbox('Select Province', options=list(province_mapping.keys()), format_func=lambda x: x, index=0)
            provincecode_value = province_mapping[provincecode]

            policestation_mapping = {
                'Parow': 'PRW104WC',
                'Athlone': 'AT03WC',
                'Bellville': 'BV08WC ',
                'Bishop Lavis': 'BL010WC',
                'Gugulethu': 'GUG49WC',
                'Khayelitsha': 'KHL57WC',
                'Manenberg': 'MBG83WC'
            }
            policestationcode = st.selectbox('Select Police Station', options=list(policestation_mapping.keys()), format_func=lambda x: x, index=0)
            policestationcode_value = policestation_mapping[policestationcode]

            year_mapping = st.slider('Select year range from 2016 - 2023', 2016, 2023)
            quarter = st.radio('Select quarter of year', options=[1, 2, 3, 4], index=0)

    if st.button('Fetch Data'):
        if not provincecode_value:
            st.error('Please select a valid province.')
        elif not policestationcode_value:
            st.error('Please select a valid police station.')
        else:
            data = fetch_data(provincecode_value, policestationcode_value, year_mapping, quarter)
            df = pd.DataFrame(data)
            st.subheader('Fetched Data:')
            st.dataframe(df)

        # Main content area
        st.header('3. Set Parameters')
        parameter_split_size = st.slider('Data split ratio (% for Training Set)', 10, 90, 80, 5)

        st.subheader('3.1. Learning Parameters')
        with st.expander('See parameters'):     
            parameter_n_estimators = st.selectbox('Number of estimators (n_estimators)', range(0, 1001, 100), index=10)
            parameter_max_features = st.selectbox('Max features (max_features)', ['all', 'sqrt', 'log2'])
            parameter_min_samples_split = st.selectbox('Minimum number of samples required to split an internal node (min_samples_split)', range(2, 11, 1), index=1)
            parameter_min_samples_leaf = st.selectbox('Minimum number of samples required to be at a leaf node (min_samples_leaf)', range(1, 11, 1), index=1)

        st.subheader('3.2. General Parameters')
        with st.expander('See parameters', expanded=False):
            parameter_random_state = st.slider('Seed number (random_state)', 0, 1000, 42, 1)
            parameter_criterion = st.select_slider('Performance measure (criterion)', options=['squared_error', 'absolute_error', 'friedman_mse'])
            parameter_bootstrap = st.select_slider('Bootstrap samples when building trees (bootstrap)', options=[True, False])
            parameter_oob_score = st.select_slider('Whether to use out-of-bag samples to estimate the R^2 on unseen data (oob_score)', options=[False, True])

        sleep_time = st.slider('Sleep time', 0, 3, 0)

# Model building and evaluation
if not df.empty:
    st.warning('ðŸ‘ˆ Upload a CSV file or click *"Load example data"* to get started!')
    with st.status("Running ...", expanded=True) as status:
        st.write("Loading initial input parameters ...")
        time.sleep(sleep_time)

        st.write("Loading outliers input parameters ...")
        time.sleep(sleep_time)

        st.write("Loading data ...")
        time.sleep(sleep_time)

        st.write("Preparing data ...")
        time.sleep(sleep_time)

        if 'CrimeCategory' in df.columns:
            df_numeric = df.drop(columns=['CrimeCategory'])
        else:
            df_numeric = df.copy()

        X = df_numeric.iloc[:, :-1]
        y = df_numeric.iloc[:, -1]

        st.write("Splitting data ...")
        time.sleep(sleep_time)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(100-parameter_split_size)/100, random_state=parameter_random_state)

        st.write("Model training ...")
        time.sleep(sleep_time)

        if parameter_max_features == 'all':
            parameter_max_features = None
            parameter_max_features_metric = X.shape[1]

        rf = RandomForestRegressor(
            n_estimators=parameter_n_estimators,
            max_features=parameter_max_features,
            min_samples_split=parameter_min_samples_split,
            min_samples_leaf=parameter_min_samples_leaf,
            random_state=parameter_random_state,
            criterion=parameter_criterion,
            bootstrap=parameter_bootstrap,
            oob_score=parameter_oob_score)
        rf.fit(X_train, y_train)

        st.write("Applying model to make predictions ...")
        time.sleep(sleep_time)
        y_train_pred = rf.predict(X_train)
        y_test_pred = rf.predict(X_test)

        st.write("Evaluating performance metrics ...")
        time.sleep(sleep_time)
        train_mse = mean_squared_error(y_train, y_train_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        test_mse = mean_squared_error(y_test, y_test_pred)
        test_r2 = r2_score(y_test, y_test_pred)

        st.subheader('Model Performance')
        st.write(f"Train MSE: {train_mse:.3f}")
        st.write(f"Train R^2: {train_r2:.3f}")
        st.write(f"Test MSE: {test_mse:.3f}")
        st.write(f"Test R^2: {test_r2:.3f}")

        # Altair chart for comparing predicted vs actual
        st.subheader('Prediction vs Actual')
        chart_data = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})
        chart = alt.Chart(chart_data).mark_circle(size=60).encode(
            x='Actual',
            y='Predicted',
            tooltip=['Actual', 'Predicted']
        ).interactive()

        st.altair_chart(chart, use_container_width=True)

        status.update("Done! Model has been built and evaluated.")
else:
    st.warning('ðŸ‘ˆ Upload a CSV file or click *"Load example data"* to get started!')
